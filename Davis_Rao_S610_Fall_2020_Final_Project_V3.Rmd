---
title: "Predicting Rihanna Song Lyrics using Markov Models for Language"
author: "Jasmine L. Davis & Shelley Rao"
date: "11/29/2020"
output: pdf_document
sansfont: Times New Roman
header-includes:
    - \usepackage{setspace}\doublespacing
    - \usepackage{float}
geometry: margin=1in
fontsize: 12pt
indent: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(tidyverse)
library(genius)
library(tidytext)
library(markovchain)
library(ggplot2)
library(msm)
library(quanteda)
options(width=80)
set.seed
```

##Get the Rihanna Album Loud
```{r}
song_data <- genius_album(artist = "Rihanna", album = "Loud", info = "all")
song_data
```
#Introduction 

In this project we decided to use Markov Models to predict the lyrics for songs in Rihanna's album "Loud". The Markov model relies heavily on probability theory. Markov models attempt to predict the probability that some event will happen in a particular sequence. The Markov models assumes that to predict the next event will be predicted from the current event.  In the case of Markov Language Models, they explore how the next word is predicted from the current word in the chain. We used the genius packages to retrieve the discographic information for Rihanna's album title "Loud." 

## Preprocessing  
So there is a debate about the pros and cons of including conjoining words like "the" "of" and "to". The pros of removing the conjoining words is that they make for a much cleaner more focused analysis. A con of removing the stop worlds is that it can result in a markov prediction model that does not have the most natural flow of language. We decided to run our models both ways to see the differences and choose the model that produces the best result.   

```{r}
loud_lyrics <- song_data %>%
  unnest(track_title)

tidy_loud <- loud_lyrics %>%
  unnest_tokens(word, lyric)
# anti_join(stop_words) #For the models with the stop words removed

tidy_loud_stop <- loud_lyrics %>%
  unnest_tokens(word, lyric) %>%
  anti_join(stop_words) #For the models with the stop words removed
```
In order to get a better understanding of the data we gathered descriptive statistics of Rihanna's Loud song lyrics. First, we take note of the frequencies of words across the various songs in the album.Then we examined the word count for each track on the Loud album!In the analysis with the stop words the top 5 most frequently used words were "yeah", "i", "oh", "you", and "the". In the analysis without the stop words the top five words were "yeah", "pum", "na", "love", and "wanna" . So we see that yeah is the same across the two but in the analysis with no stop words "pum", "na", "love", and "wanna" were 16th, 17th, 19th, 25th, respectively. 

In the model with the stop words. We the word "Yeah" is concentrated with the song "Cheers(Drink to That" and "you" and "I" are spread across the songs on the album. In the model with the stop words removed we see that Yeah is still concentrated in "Cheers", we see that "pum" is concentrated in"Man Down",and that "na" is mostly seen in the song "SM". Love is concentrated across four songs on the album: "only girl", "SM", "Love the way you lie", and "Complicated".

```{r}
# Frequency of each word across all songs 
tidy_loud %>%
  count(word, sort = TRUE)

# Count words per song
words_by_track_title <- tidy_loud %>%
  count(word, track_title,  sort = TRUE) %>%
  ungroup()
words_by_track_title

```
#Plots for Frequency of Words
```{r}

library(ggplot2)

tidy_loud %>%
  count(word, sort = TRUE) %>%
  filter(n > 25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)

library(ggplot2)

tidy_loud_stop %>%
  count(word, sort = TRUE) %>%
  filter(n > 25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)

#Question for Fukuyama: Is there a way to plot this with words on y axis and songs on the x axis to see which words appear in which song?

```
##Word clloud to show frequency of words 
```{r}
library(wordcloud)

tidy_loud %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```
After exploring the word frequency we see that despite the between the data with stop words and the one without. The data without the stop words fared better. The words included in the frequency plots were more telling of the language used in the album than the data with the stop words.So for the sentiment analysis and adjusted frequency we will be using the data with the stop words remover "tidy_loud_stop"

In our preliminary analysis before our prediction models we performed sentiment analysis or opinion mining on the "Loud" album. We also calculated the adjusted frequency of words to understand their important in the album. For sentiment analysis we are using the commonly used approach, which vies the text and sentiment as a sum of the individual words and sentiment content of those individual words. We used the bing sentiment lexicon for the simple negative and positive rating of words in the album. 

#Sentiment Analysis

In our sentiment analysis we see that the largest frequncy of work in the album are positive. This is shown in the world cloud showing that Love and easy are the largest words in the cloud. The most frequent negative words are complicated and lie. We then went on to see what percentage of words in each song were negative and positive. The songs with the highest ratio of negative words were the songs "Complicated" and "Love the way you Lie". The songs with the highest ratio of positive words are "S&M", "Only girl", "Love the way you Lie", and "Complicated".
```{r}

#sentiment word cloud
library(reshape2)
tidy_loud_stop %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```
#sentiment by track
```{r}

#negative
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_loud_stop %>%
  group_by(track_title) %>%
  summarize(words = n())

tidy_loud %>%
  semi_join(bingnegative) %>%
  group_by(track_title) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("track_title")) %>%
  mutate(ratio = negativewords/words) %>%
  ungroup()

#positive
bingpositive <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")

wordcounts <- tidy_loud_stop%>%
  group_by(track_title) %>%
  summarize(words = n())

tidy_loud %>%
  semi_join(bingpositive) %>%
  group_by(track_title) %>%
  summarize(positivewords = n()) %>%
  left_join(wordcounts, by = c("track_title")) %>%
  mutate(ratio = positivewords/words)
```

#Adjusted frequency of Words

Before running our Markov models we ar attempting to use statistical methods to quantify what our song is about. We used word frequency, sentiment analysis, and now we are exploring how important a word is by exploring its adjusted frequency. The tf-idf looks at the frequency of a term adjusted by how rarely it is used. Below we calculated the tf, idf, and adjusted frequency for each word in the album "Loud". We see that the words "yeah", "pum" , and "na" are deemed important because they are frequent words but are not seen across all the tracks like filler works like"in" and "so". Adjust frequency or the tf-idf terms allows for us to find the most important words without dealing with filler words because those words are weighted down because they occur more frequently. 

```{r}
tf_idf <- words_by_track_title %>%
  bind_tf_idf(word, track_title, n) %>%
  arrange(desc(tf_idf))
tf_idf

```

#Pairwise Correlation

Lastly, before setting up for our Markov prediction model we looked at how the tracks of the album are correlated. We see that Love the Way you Lie, Raining Men, and Whats my Name are the most higly correlated songs. 

```{r}
# What songs tend to have similar content?
library(widyr)
track_title_cors <- words_by_track_title %>%
  pairwise_cor(track_title, word, n, sort = TRUE)
track_title_cors

```

# Building the Transition Matrix

In order to run a markov prediction model we first have to build a transition matrix. A transition matrix is a square matrix that gives the probability of moving from one states to another. In our case we are building a matrix that predicts going from one word to the next given the chain of song lyrics.
```{r}
library(tidyr)
# create vector of words from Rihanna's Loud  lyrics
words <- (tidy_loud_stop[,8])
# create transition matrix
d2 <- data.frame(do.call(rbind, 
                         lapply(words , 
                                function(x) t(sapply(1:(length(x) - 1), 
                                        function(i) c(x[i], x[i+1]))))))
tr.mat <- table(d2[,1], d2[,2])
# make the matrix row-stochastic
transition.matrix <- tr.mat / rowSums(tr.mat) 
# assign row and column names
rownames0 <- dimnames(transition.matrix)[[1]]
rownames(transition.matrix) <- rownames0
colnames0 <- dimnames(transition.matrix)[[2]]
colnames(transition.matrix) <- colnames0
```

# Next Word Prediction

Finally, we predict the next word based on a randomly selected starting word. In order to perform a markov predictive model we write a function that first randomly selects a starting word then uses the the transition matrix to predict the second word. the markov prediction function takes three arguments: n, transition.matrix, and start. "n" is the number of probabilities sample, then our transition matrix, and the start word. The function prints the starting word and the second word predicted by the markov model.  

In the model with the stop words included we got little last as the word. I put "rihanna lyrics litte last" into a search engine and those two word are from the track california king bed on the loud album. SO the model without the stop words worked correctly. In the model with the stop words removed we got "hero" and "lost" upon searching these lyrics it is from the track love the way you lie but these two words are not in the lyrics together. The lyrics read "But you'll always be my hero. Even though you've lost your mind" So we see here that eliminating the stop words actually make it much harder to create natural predictions
```{r}
rmarkovchain <- function(n, transition.matrix, start) {
  #randomly select a starting word
  start <- sample(1:nrow(transition.matrix), 1) 
  #create empty character to paste start and predicted words
  word_pred <- character()
  #create empty vector
  output <- rep (NA, n)
  #replace first element of vector with starting word
  output[1] <- start
  #select next word based on previous ("starting") word
  for (i in 2:n) {
    output[i] <- 
    sample(ncol(transition.matrix), 1, 
           prob=transition.matrix[output[i - 1],])
    }
  rownames0 = rownames(transition.matrix)
  return(rownames0[output])

}
print(rmarkovchain(n = 10, transition.matrix)) 
```
# TESTING ON A SMALLER SAMPLE

Since an entire album is a very large text, we tested our function on a smaller set of text just to make sure everything was working correctly. We chose an excerpt from the song "Watermelon Sugar" by Harry Styles. As you can see below the Markov Function is correctly predicting the words. We got "just thinking" from the model and that is how it appears in the text excerpt. 
```{r}
harry <- "Breathe me in, breathe me out
I don't know if I could ever go without
I'm just thinking out loud
I don't know if I could ever go without
Watermelon sugar high"
harry.1 <- unlist(strsplit(harry, "\\s+"))
harry.2 <- data.frame(do.call(rbind, 
                         lapply(harry.1 , 
                                function(x) t(sapply(1:(length(x) - 1), 
                                        function(i) c(x[i], x[i+1]))))))
harry.2 <- matrix(c(harry.1), nrow=16, ncol=2, byrow=TRUE)
tr.mat.test <- table(harry.2[,1], harry.2[,2])

# make the matrix row-stochastic
transition.matrix.test <- tr.mat.test / rowSums(tr.mat.test) 

# assign row and column names
rownames0 <- dimnames(transition.matrix.test)[[1]]
rownames(transition.matrix.test) <- rownames0
colnames0 <- dimnames(transition.matrix.test)[[2]]
colnames(transition.matrix.test) <- colnames0
```

```{r}

# next work prediction
rmarkovchain_test <- function(n, transition.matrix.test, start) {
#randomly select a starting word
start_test <- sample(1:nrow(transition.matrix.test), 1) 
#create empty character to paste start and predicted words
word_pred_test <- character()
#create empty vector
output_test <- rep (NA, n)
#replace first element of vector with starting word
output_test[1] <- start_test
#select next word based on previous ("starting") word
for (i in 2:n) {
output_test[i] <- sample(ncol(transition.matrix.test), 1, 
prob=transition.matrix.test[output_test[i - 1],])
    }
rownames0 = rownames(transition.matrix.test)
  return(rownames0[output_test])
}
print(rmarkovchain(n = 7, transition.matrix.test)) 
```

#Appendix A: Markov Model with Stop Words Included
```{r}
library(tidyr)
# create vector of words from Rihanna's Loud  lyrics
words_alt <- (tidy_loud[,8])
# create transition matrix
d2_alt <- data.frame(do.call(rbind, 
                         lapply(words_alt , 
                                function(x) t(sapply(1:(length(x) - 1), 
                                        function(i) c(x[i], x[i+1]))))))
tr.mat.alt <- table(d2_alt[,1], d2_alt[,2])
# make the matrix row-stochastic
transition.matrix.alt<- tr.mat.alt / rowSums(tr.mat.alt) 
# assign row and column names
rownames0 <- dimnames(transition.matrix.alt)[[1]]
rownames(transition.matrix.alt) <- rownames0
colnames0 <- dimnames(transition.matrix.alt)[[2]]
colnames(transition.matrix.alt) <- colnames0
```

```{r}
rmarkovchain_alt <- function(n, transition.matrix.alt, start_alt) {
  #randomly select a starting word
  start_alt <- sample(1:nrow(transition.matrix.alt), 1) 
  #create empty character to paste start and predicted words
  word_pred_alt<- character()
  #create empty vector
  output_alt <- rep (NA, n)
  #replace first element of vector with starting word
  output_alt[1] <- start_alt
  #select next word based on previous ("starting") word
  for (i in 2:n) {
    output[i] <- 
    sample(ncol(transition.matrix.alt), 1, 
           prob=transition.matrix.alt[output_alt[i - 1],])
    }
  rownames0 = rownames(transition.matrix.alt)
  return(rownames0[output_alt])
}
print(rmarkovchain(n = 2, transition.matrix)) 



```

